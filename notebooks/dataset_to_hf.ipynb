{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-25T11:46:48.946219926Z",
     "start_time": "2023-07-25T11:46:48.945290507Z"
    }
   },
   "outputs": [],
   "source": [
    "# train/val/test -> 60% / 20% / 20%, stratified by number of answers\n",
    "from tqdm.notebook import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13108\n",
      "{'question': 'Name A Number Thatâ€™s Associated With Being Lucky Or Unlucky', 'link': '/question/name-a-number-thats-associated-with-being-lucky-or-unlucky', 'answers': [{'text': '7', 'points': 68}, {'text': '13', 'points': 26}, {'text': '3', 'points': 5}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('../data_store/question_db.json', 'r') as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "print(len(records))\n",
    "print(records[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T11:32:52.289937597Z",
     "start_time": "2023-07-25T11:32:52.249917142Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /app/.cache/huggingface/datasets/generator/default-306605e0dbff30a0/0.0.0...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d6d029ac77d4e168612a3754c72a235"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/datasets/builder.py:1629\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._prepare_split_single\u001B[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001B[0m\n\u001B[1;32m   1628\u001B[0m _time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m-> 1629\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, record \u001B[38;5;129;01min\u001B[39;00m generator:\n\u001B[1;32m   1630\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_shard_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m writer\u001B[38;5;241m.\u001B[39m_num_bytes \u001B[38;5;241m>\u001B[39m max_shard_size:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/datasets/packaged_modules/generator/generator.py:30\u001B[0m, in \u001B[0;36mGenerator._generate_examples\u001B[0;34m(self, **gen_kwargs)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_examples\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgen_kwargs):\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m idx, ex \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgen_kwargs\u001B[49m\u001B[43m)\u001B[49m):\n\u001B[1;32m     31\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m idx, ex\n",
      "\u001B[0;31mTypeError\u001B[0m: 'list' object is not callable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mDatasetGenerationError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[0;32m----> 3\u001B[0m d \u001B[38;5;241m=\u001B[39m \u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecords\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py:1052\u001B[0m, in \u001B[0;36mDataset.from_generator\u001B[0;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, **kwargs)\u001B[0m\n\u001B[1;32m   1004\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Create a Dataset from a generator.\u001B[39;00m\n\u001B[1;32m   1005\u001B[0m \n\u001B[1;32m   1006\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1048\u001B[0m \u001B[38;5;124;03m```\u001B[39;00m\n\u001B[1;32m   1049\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgenerator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GeneratorDatasetInputStream\n\u001B[0;32m-> 1052\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mGeneratorDatasetInputStream\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1053\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1054\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1055\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1056\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1057\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgen_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgen_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1058\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1059\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1060\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/datasets/io/generator.py:47\u001B[0m, in \u001B[0;36mGeneratorDatasetInputStream.read\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     44\u001B[0m     verification_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     base_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# try_from_hf_gcs=try_from_hf_gcs,\u001B[39;49;00m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbase_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mas_dataset(\n\u001B[1;32m     56\u001B[0m         split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, verification_mode\u001B[38;5;241m=\u001B[39mverification_mode, in_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkeep_in_memory\n\u001B[1;32m     57\u001B[0m     )\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/datasets/builder.py:909\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[1;32m    907\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    908\u001B[0m         prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[0;32m--> 909\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    910\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    911\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    912\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_split_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    913\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdownload_and_prepare_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    914\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    915\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/datasets/builder.py:1670\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001B[0m\n\u001B[1;32m   1669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_download_and_prepare\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager, verification_mode, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs):\n\u001B[0;32m-> 1670\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1673\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_duplicate_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mVerificationMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBASIC_CHECKS\u001B[49m\n\u001B[1;32m   1674\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mVerificationMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mALL_CHECKS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1675\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_splits_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1676\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/datasets/builder.py:1004\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[1;32m   1000\u001B[0m split_dict\u001B[38;5;241m.\u001B[39madd(split_generator\u001B[38;5;241m.\u001B[39msplit_info)\n\u001B[1;32m   1002\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1003\u001B[0m     \u001B[38;5;66;03m# Prepare split will record examples associated to the split\u001B[39;00m\n\u001B[0;32m-> 1004\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_generator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_split_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1006\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m   1007\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find data file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1008\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_download_instructions \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1009\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mOriginal error:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1010\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)\n\u001B[1;32m   1011\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/datasets/builder.py:1508\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._prepare_split\u001B[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001B[0m\n\u001B[1;32m   1506\u001B[0m job_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pbar:\n\u001B[0;32m-> 1508\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m job_id, done, content \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_split_single(\n\u001B[1;32m   1509\u001B[0m         gen_kwargs\u001B[38;5;241m=\u001B[39mgen_kwargs, job_id\u001B[38;5;241m=\u001B[39mjob_id, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_prepare_split_args\n\u001B[1;32m   1510\u001B[0m     ):\n\u001B[1;32m   1511\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[1;32m   1512\u001B[0m             result \u001B[38;5;241m=\u001B[39m content\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/datasets/builder.py:1665\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._prepare_split_single\u001B[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001B[0m\n\u001B[1;32m   1663\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, SchemaInferenceError) \u001B[38;5;129;01mand\u001B[39;00m e\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1664\u001B[0m         e \u001B[38;5;241m=\u001B[39m e\u001B[38;5;241m.\u001B[39m__context__\n\u001B[0;32m-> 1665\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetGenerationError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while generating the dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   1667\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m job_id, \u001B[38;5;28;01mTrue\u001B[39;00m, (total_num_examples, total_num_bytes, writer\u001B[38;5;241m.\u001B[39m_features, num_shards, shard_lengths)\n",
      "\u001B[0;31mDatasetGenerationError\u001B[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T11:34:28.778213672Z",
     "start_time": "2023-07-25T11:34:25.702389501Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "d = Dataset.from_list(records)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T11:36:06.521706169Z",
     "start_time": "2023-07-25T11:36:06.471339416Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# split in answers/points\n",
    "# train/val/test\n",
    "# save json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T11:42:44.613862483Z",
     "start_time": "2023-07-25T11:42:44.613148802Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/13108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff3bb6b3111246099cf006f900e55d9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for record in records:\n",
    "'question'\n",
    "'answers'\n",
    "'has_points'\n",
    "'points'\n",
    "\n",
    "def process(record):\n",
    "    new_record = {}\n",
    "    new_record['question'] = record['question']\n",
    "    new_record['answers'] = [a['text'] for a in record['answers']]\n",
    "    new_record['points'] = [a['points'] for a in record['answers']]\n",
    "    new_record['has_points'] = record['answers'][0]['points'] is not None\n",
    "    return new_record\n",
    "\n",
    "records2 = [process(r) for r in tqdm(records)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T11:46:54.884582325Z",
     "start_time": "2023-07-25T11:46:54.474152226Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T11:48:15.269329765Z",
     "start_time": "2023-07-25T11:48:14.962560965Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T11:49:48.719750248Z",
     "start_time": "2023-07-25T11:49:48.681020083Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "num_answers = [(len(r['answers']), r['has_points']) for r in records2]\n",
    "train, test = train_test_split(records2, test_size=0.2, stratify=num_answers)\n",
    "num_answers = [(len(r['answers']), r['has_points']) for r in train]\n",
    "train, val = train_test_split(train, test_size=0.25, stratify=num_answers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T11:54:05.436974068Z",
     "start_time": "2023-07-25T11:54:05.382507999Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "\n",
    "# plt.hist([len(r['answers']) for r in train], alpha=0.7)\n",
    "# plt.hist([len(r['answers']) for r in val], alpha=0.7)\n",
    "# plt.hist([len(r['answers']) for r in test], alpha=0.7)\n",
    "# l\n",
    "\n",
    "for split, ds in {'train':train, 'val':val, 'test':test}.items():\n",
    "    with open(f'../data_store/{split}.json', 'w') as f:\n",
    "        json.dump(ds, f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T11:56:11.535156576Z",
     "start_time": "2023-07-25T11:56:11.331205282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/252 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c45a3ed993c45a8af3672e90f9e6b13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/fedyanin--feud to /app/.cache/huggingface/datasets/fedyanin___json/fedyanin--feud-ae61a2ce32f00c43/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "571f272cd0b8434ca24c2c277d5c958e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/1.64M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e55c9fef70db448785aad5c9f367570d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/546k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cf982d655ad45b49c7d3d060c56329f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/545k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e298e6046b14263ae967ce71798a062"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09903e02a23b4e1cb87c04d5cc0d49d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6c39184840645efb4b257fe64e9c92f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbf83c811298442e83fb10b73737498b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "400ceede6a0c413782e51b52abbdabc7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /app/.cache/huggingface/datasets/fedyanin___json/fedyanin--feud-ae61a2ce32f00c43/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d866b5fd9284e2eaddbeac1709f5ad0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"fedyanin/feud\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:06:18.569180033Z",
     "start_time": "2023-07-25T12:06:08.019441106Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "tr = dataset['train']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:06:23.015807562Z",
     "start_time": "2023-07-25T12:06:23.013858672Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "{'question': 'Name something men say they hate to give up when they get married',\n 'has_points': False,\n 'answers': ['Emancipation',\n  'Celebrating',\n  'Autonomy',\n  'Liberty',\n  'Drinking',\n  'Freedom'],\n 'points': [None, None, None, None, None, None]}"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-25T12:06:44.270485503Z",
     "start_time": "2023-07-25T12:06:44.269463552Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
