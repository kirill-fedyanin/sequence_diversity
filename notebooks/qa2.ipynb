{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "RWForCausalLM(\n  (transformer): RWModel(\n    (word_embeddings): Embedding(65024, 4544)\n    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n        (self_attention): Attention(\n          (maybe_rotary): RotaryEmbedding()\n          (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)\n          (dense): Linear(in_features=4544, out_features=4544, bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n          (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n) is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:687\u001B[0m, in \u001B[0;36mget_file_from_repo\u001B[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001B[0m\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    686\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 687\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresolved_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    689\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    693\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    695\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:284\u001B[0m, in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_url(url_or_filename):\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001B[39;00m\n\u001B[0;32m--> 284\u001B[0m     output_path \u001B[38;5;241m=\u001B[39m \u001B[43mget_from_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_or_filename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(url_or_filename):\n\u001B[1;32m    295\u001B[0m     \u001B[38;5;66;03m# File, and it exists.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:495\u001B[0m, in \u001B[0;36mget_from_cache\u001B[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m    494\u001B[0m r \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mhead(url, headers\u001B[38;5;241m=\u001B[39mheaders, allow_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, proxies\u001B[38;5;241m=\u001B[39mproxies, timeout\u001B[38;5;241m=\u001B[39metag_timeout)\n\u001B[0;32m--> 495\u001B[0m \u001B[43m_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    496\u001B[0m etag \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX-Linked-Etag\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m r\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mETag\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:417\u001B[0m, in \u001B[0;36m_raise_for_status\u001B[0;34m(response)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m401\u001B[39m:\n\u001B[1;32m    416\u001B[0m     \u001B[38;5;66;03m# The repo was not found and the user is not Authenticated\u001B[39;00m\n\u001B[0;32m--> 417\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RepositoryNotFoundError(\n\u001B[1;32m    418\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m401 Client Error: Repository not found for url: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    419\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf the repo is private, make sure you are authenticated.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    420\u001B[0m     )\n\u001B[1;32m    422\u001B[0m response\u001B[38;5;241m.\u001B[39mraise_for_status()\n",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m: 401 Client Error: Repository not found for url: https://huggingface.co/RWForCausalLM(%0A%20%20(transformer):%20RWModel(%0A%20%20%20%20(word_embeddings):%20Embedding(65024,%204544)%0A%20%20%20%20(h):%20ModuleList(%0A%20%20%20%20%20%20(0-31):%2032%20x%20DecoderLayer(%0A%20%20%20%20%20%20%20%20(input_layernorm):%20LayerNorm((4544,),%20eps=1e-05,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20(self_attention):%20Attention(%0A%20%20%20%20%20%20%20%20%20%20(maybe_rotary):%20RotaryEmbedding()%0A%20%20%20%20%20%20%20%20%20%20(query_key_value):%20Linear(in_features=4544,%20out_features=4672,%20bias=False)%0A%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=4544,%20out_features=4544,%20bias=False)%0A%20%20%20%20%20%20%20%20%20%20(attention_dropout):%20Dropout(p=0.0,%20inplace=False)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(mlp):%20MLP(%0A%20%20%20%20%20%20%20%20%20%20(dense_h_to_4h):%20Linear(in_features=4544,%20out_features=18176,%20bias=False)%0A%20%20%20%20%20%20%20%20%20%20(act):%20GELU(approximate='none')%0A%20%20%20%20%20%20%20%20%20%20(dense_4h_to_h):%20Linear(in_features=18176,%20out_features=4544,%20bias=False)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20)%0A%20%20%20%20)%0A%20%20%20%20(ln_f):%20LayerNorm((4544,),%20eps=1e-05,%20elementwise_affine=True)%0A%20%20)%0A%20%20(lm_head):%20Linear(in_features=4544,%20out_features=65024,%20bias=False)%0A)/resolve/main/tokenizer_config.json. If the repo is private, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtiiuae/falcon-7b\u001B[39m\u001B[38;5;124m\"\u001B[39m, trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# model = 'tiiuae/falcon-7b'\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# pipeline = transformers.pipeline(\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m#     \"text-generation\",\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#     model=model,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# for seq in sequences:\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m#     print(f\"Result: {seq['generated_text']}\")\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:522\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    521\u001B[0m \u001B[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001B[39;00m\n\u001B[0;32m--> 522\u001B[0m tokenizer_config \u001B[38;5;241m=\u001B[39m \u001B[43mget_tokenizer_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    523\u001B[0m config_tokenizer_class \u001B[38;5;241m=\u001B[39m tokenizer_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer_class\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    524\u001B[0m tokenizer_auto_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:380\u001B[0m, in \u001B[0;36mget_tokenizer_config\u001B[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_tokenizer_config\u001B[39m(\n\u001B[1;32m    312\u001B[0m     pretrained_model_name_or_path: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike],\n\u001B[1;32m    313\u001B[0m     cache_dir: Optional[Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    321\u001B[0m ):\n\u001B[1;32m    322\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;124;03m    Loads the tokenizer configuration from a pretrained model tokenizer configuration.\u001B[39;00m\n\u001B[1;32m    324\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;124;03m    tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001B[39;00m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;124;03m    ```\"\"\"\u001B[39;00m\n\u001B[0;32m--> 380\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mget_file_from_repo\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mTOKENIZER_CONFIG_FILE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    389\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    392\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:698\u001B[0m, in \u001B[0;36mget_file_from_repo\u001B[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001B[0m\n\u001B[1;32m    687\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m cached_path(\n\u001B[1;32m    688\u001B[0m         resolved_file,\n\u001B[1;32m    689\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    694\u001B[0m         use_auth_token\u001B[38;5;241m=\u001B[39muse_auth_token,\n\u001B[1;32m    695\u001B[0m     )\n\u001B[1;32m    697\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError:\n\u001B[0;32m--> 698\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    699\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a local folder and is not a valid model identifier \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    700\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlisted on \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf this is a private repository, make sure to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    701\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    702\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    703\u001B[0m     )\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RevisionNotFoundError:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor this model name. Check the model page at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    708\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for available revisions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    709\u001B[0m     )\n",
      "\u001B[0;31mOSError\u001B[0m: RWForCausalLM(\n  (transformer): RWModel(\n    (word_embeddings): Embedding(65024, 4544)\n    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n        (self_attention): Attention(\n          (maybe_rotary): RotaryEmbedding()\n          (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)\n          (dense): Linear(in_features=4544, out_features=4544, bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n          (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n) is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\", trust_remote_code=True)\n",
    "# model = 'tiiuae/falcon-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"cuda:0\",\n",
    "# )\n",
    "\n",
    "\n",
    "# sequences = pipeline(\n",
    "#    \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "#     max_length=200,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:34:02.361578906Z",
     "start_time": "2023-06-02T11:32:49.555548934Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "RWForCausalLM(\n  (transformer): RWModel(\n    (word_embeddings): Embedding(65024, 4544)\n    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n        (self_attention): Attention(\n          (maybe_rotary): RotaryEmbedding()\n          (query_key_value): Linear(in_features=4544, out_features=4672, bias=False)\n          (dense): Linear(in_features=4544, out_features=4544, bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear(in_features=4544, out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n          (dense_4h_to_h): Linear(in_features=18176, out_features=4544, bias=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:35:58.086136305Z",
     "start_time": "2023-06-02T11:35:54.579615449Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "question_text = 'Who is you bro?'\n",
    "sequence = tokenizer(question_text, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:35:21.298577889Z",
     "start_time": "2023-06-02T11:35:21.260118003Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_return_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m               \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:1320\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001B[0m\n\u001B[1;32m   1312\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[1;32m   1313\u001B[0m         input_ids,\n\u001B[1;32m   1314\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mnum_return_sequences,\n\u001B[1;32m   1315\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[1;32m   1316\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1317\u001B[0m     )\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;66;03m# 12. run sample\u001B[39;00m\n\u001B[0;32m-> 1320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1321\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogits_warper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1326\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1327\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1328\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1329\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1330\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1331\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1333\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_beam_gen_mode:\n\u001B[1;32m   1334\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_return_sequences \u001B[38;5;241m>\u001B[39m num_beams:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.8/site-packages/transformers/generation_utils.py:1935\u001B[0m, in \u001B[0;36mGenerationMixin.sample\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001B[0m\n\u001B[1;32m   1932\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m   1934\u001B[0m \u001B[38;5;66;03m# prepare model inputs\u001B[39;00m\n\u001B[0;32m-> 1935\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_inputs_for_generation\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1937\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[1;32m   1938\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(\n\u001B[1;32m   1939\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs,\n\u001B[1;32m   1940\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   1941\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m   1942\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m   1943\u001B[0m )\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/da8d49a4c7dde3bfc39461e6f2cf7433e2fa44c2/modelling_RW.py:712\u001B[0m, in \u001B[0;36mRWForCausalLM.prepare_inputs_for_generation\u001B[0;34m(self, input_ids, past, attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m    710\u001B[0m     \u001B[38;5;66;03m# the cache may be in the stardard format (e.g. in contrastive search), convert to our's format if needed\u001B[39;00m\n\u001B[1;32m    711\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m past[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[0;32m--> 712\u001B[0m         past \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_to_rw_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpast\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    714\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m    715\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m: input_ids,\n\u001B[1;32m    716\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpast_key_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: past,\n\u001B[1;32m    717\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m: kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    718\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: attention_mask,\n\u001B[1;32m    719\u001B[0m }\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-7b/da8d49a4c7dde3bfc39461e6f2cf7433e2fa44c2/modelling_RW.py:478\u001B[0m, in \u001B[0;36mRWPreTrainedModel._convert_to_rw_cache\u001B[0;34m(past_key_value)\u001B[0m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    475\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_convert_to_rw_cache\u001B[39m(\n\u001B[1;32m    476\u001B[0m     past_key_value: Tuple[Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]]\n\u001B[1;32m    477\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]]:\n\u001B[0;32m--> 478\u001B[0m     batch_size, num_heads, head_dim, seq_length \u001B[38;5;241m=\u001B[39m past_key_value[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    479\u001B[0m     batch_size_times_num_heads \u001B[38;5;241m=\u001B[39m batch_size \u001B[38;5;241m*\u001B[39m num_heads\n\u001B[1;32m    480\u001B[0m     \u001B[38;5;66;03m# key:  [batch_size, num_heads, head_dim, seq_length] -> [batch_size * num_heads, head_dim, seq_length]\u001B[39;00m\n\u001B[1;32m    481\u001B[0m     \u001B[38;5;66;03m# value: [batch_size, num_heads, seq_length, head_dim] -> [batch_size * num_heads, seq_length, head_dim]\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "\n",
    "model.generate(\n",
    "    sequence,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T11:38:01.474817128Z",
     "start_time": "2023-06-02T11:38:00.864197659Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
