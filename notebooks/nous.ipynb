{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:05:13.148873592Z",
     "start_time": "2023-07-24T11:05:12.772440732Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\"NousResearch/Nous-Hermes-13b\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Nous-Hermes-13b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f757e9330b864fdb82a6871108496502"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "787fbe980827434e8854241bbbf42a07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed8ad8c975f44b75ae89dbf5a10c0b82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)in/added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c34179d2364b466bb0ec3b3ce3e5ee95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03e0d5237a634d9cac1df1df607fb067"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "800d41f263ed4abdb58260cfe621d3e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aef23f0d4e8c4c5bbf1d225ba93f6c0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d17c2894ac644e859805cd1df665a56e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eba18c051ebf427c9c53fb60f469be49"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9ae38805bd4467187bed72239ed7344"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9026987d4e4144baa1e8d544bfa650d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16b124b8bbec49bb89e9639a0d1ac7da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T11:14:51.238156693Z",
     "start_time": "2023-07-24T11:07:03.467856738Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: if we calculate maximum depth of binary tree, with node not having info about parent, is it possible to do sub-linear memory worse case?\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "def template(input, instruction=\"You are smart and helpful chatbot\"):\n",
    "    return  f\"\"\"### Instruction: {instruction}\\n\\n### Input: {input}\\n\\n### Response:\"\"\"\n",
    "\n",
    "prompt = template('if we calculate maximum depth of binary tree, with node not having info about parent, is it possible to do sub-linear memory worse case?')\n",
    "\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:04:33.682231447Z",
     "start_time": "2023-07-12T12:04:33.676968223Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32001, 5120, padding_idx=0)\n    (layers): ModuleList(\n      (0-39): 40 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=5120, out_features=32001, bias=False)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:04:41.277299722Z",
     "start_time": "2023-07-12T12:04:33.679967986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: in which countries name natan is popular\n",
      "\n",
      "### Response:Natan is a Hebrew name that is popular in Israel and among Jewish communities around the world. It is also used in some Christian communities.</s>\n"
     ]
    }
   ],
   "source": [
    "# prompt = template('if we calculate maximum depth of binary tree, with node not having info about parent, is it possible to do sub-linear memory worse case?')\n",
    "# prompt = template('does \"vignes naturelles\" on the wine bottle is reasonable? what does it even mean?')\n",
    "prompt = template('in which countries name natan is popular')\n",
    "response = model.generate(tokenizer.encode(prompt, return_tensors='pt').cuda(), max_new_tokens=500)\n",
    "print(tokenizer.decode(response[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:04:42.668008640Z",
     "start_time": "2023-07-12T12:04:41.308124480Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def answer(text):\n",
    "    prompt = template(text)\n",
    "    response = model.generate(tokenizer.encode(prompt, return_tensors='pt').cuda(), max_new_tokens=500)\n",
    "    print(tokenizer.decode(response[0]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:04:42.868831581Z",
     "start_time": "2023-07-12T12:04:42.676598865Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: как называется загс в германии?\n",
      "\n",
      "### Response:The name of the German parliament is the Bundestag.</s>\n"
     ]
    }
   ],
   "source": [
    "answer('как называется загс в германии?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:04:43.337340052Z",
     "start_time": "2023-07-12T12:04:42.868614767Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: what people would answer to inshallah? Are there any canonical response\n",
      "\n",
      "### Response:Inshallah is an Arabic phrase that means \"God willing.\" It is often used to express hope or a desire for something to happen. People may answer to \"Inshallah\" in different ways depending on the context of the situation. Some common responses could be \"Ameen\" (meaning \"so be it\"), \"I hope so\" or \"May God make it happen.\" There is no one canonical response to \"Inshallah,\" as it is more of a way of expressing hope or desire rather than a question that requires a specific answer.</s>\n"
     ]
    }
   ],
   "source": [
    "answer('what people would answer to inshallah? Are there any canonical response')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:04:48.502902354Z",
     "start_time": "2023-07-12T12:04:43.337158318Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: Write a short poem about girl named ellen from tunisia and tanzania who loves to dance and dreams about travel around world\n",
      "\n",
      "### Response:In Tunisia, a land of beauty and grace,\n",
      "Lived a girl named Ellen, with a heart full of space,\n",
      "For dancing, she'd twirl, with a joyful grin,\n",
      "And dream of traveling, to places far and thin.\n",
      "\n",
      "Tanzania's hills, she longed to explore,\n",
      "With each step, she'd grow more and more,\n",
      "Her spirit wild, her heart full of fire,\n",
      "A dancer, a dreamer, a girl beyond compare.\n",
      "\n",
      "So let her dance, and let her roam,\n",
      "For Ellen's heart, it knows no home,\n",
      "Let her travel, and let her twirl,\n",
      "For in this world, she'll leave a mark, unfurled.</s>\n"
     ]
    }
   ],
   "source": [
    "answer('Write a short poem about girl named ellen from tunisia and tanzania who loves to dance and dreams about travel around world')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:04:55.788649327Z",
     "start_time": "2023-07-12T12:04:48.503595501Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: could you estimate calories for cesar chicken salad\n",
      "\n",
      "### Response:Certainly! Cesar chicken salad typically contains around 350-450 calories per serving. However, this can vary depending on the specific recipe and portion size.</s>\n"
     ]
    }
   ],
   "source": [
    "answer('could you estimate calories for cesar chicken salad')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:04:57.695391264Z",
     "start_time": "2023-07-12T12:04:55.803307282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: what is key improvements in llama compare to gpt-2?\n",
      "\n",
      "### Response:GPT-2 and Llama are both powerful language models, but they have some key differences in terms of their architecture and capabilities. Here are some of the main improvements that Llama offers over GPT-2:\n",
      "\n",
      "1. Llama is a unidirectional language model, meaning that it generates text from left to right, unlike GPT-2 which is bidirectional. This allows Llama to generate more coherent and fluent text.\n",
      "\n",
      "2. Llama is based on a transformer architecture, which allows it to process input data in parallel, making it faster and more efficient than GPT-2.\n",
      "\n",
      "3. Llama has a larger training dataset, which allows it to generate more accurate and diverse text.\n",
      "\n",
      "4. Llama has a more advanced context-based language generation mechanism, which allows it to generate text that is more relevant to the input prompt.\n",
      "\n",
      "Overall, while GPT-2 is a powerful language model, Llama offers some significant improvements in terms of architecture, speed, and accuracy, making it a more advanced and effective tool for natural language generation.</s>\n"
     ]
    }
   ],
   "source": [
    "answer('what is key improvements in llama compare to gpt-2?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:05:08.307658958Z",
     "start_time": "2023-07-12T12:04:57.699311268Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "### Response:In April, Natalia sold 48 clips to her friends. In May, she sold half of that amount, which is 24 clips. Therefore, she sold a total of 48 + 24 = 72 clips in April and May.</s>\n"
     ]
    }
   ],
   "source": [
    "answer(\"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:05:11.039392988Z",
     "start_time": "2023-07-12T12:05:08.310365136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "\n",
      "### Response:Weng earned $6.00 for 50 minutes of babysitting.</s>\n"
     ]
    }
   ],
   "source": [
    "answer(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:07:58.391962Z",
     "start_time": "2023-07-12T12:07:57.472906994Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
      "\n",
      "### Response:Betty needs to save an additional $50 to have the full amount needed to buy the wallet.</s>\n"
     ]
    }
   ],
   "source": [
    "answer(\"Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:08:20.111264292Z",
     "start_time": "2023-07-12T12:08:18.976735587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Instruction: You are smart and helpful chatbot\n",
      "\n",
      "### Input: Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\n",
      "\n",
      "### Response:Julie read 12 pages yesterday and 24 pages today (twice as many). So, she has read 36 pages in total. To read half of the remaining pages, she needs to read 36 pages * (1/2) = 18 pages tomorrow.</s>\n"
     ]
    }
   ],
   "source": [
    "answer(\"Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-12T12:09:03.939785579Z",
     "start_time": "2023-07-12T12:09:00.980887370Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
